{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.preprocessing as skp\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Source & Target Dataset\n",
    "X = np.load(\"Data/olivetti_faces.npy\")\n",
    "Y = np.load(\"Data/olivetti_faces_target.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into Train & Test Dataset\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 64, 64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 64, 64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattening & Reshaping Data\n",
    "X_train = X_train.flatten().reshape(300,4096)\n",
    "X_test = X_test.flatten().reshape(100,4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.95935017, -0.5766609 , -0.40062758, ...,  0.92574406,\n",
       "         0.9085854 ,  0.7555955 ],\n",
       "       [-1.6723652 , -1.9348845 , -0.78612095, ...,  0.90439725,\n",
       "         1.1295806 ,  1.3207661 ],\n",
       "       [ 0.7886866 ,  0.82537645,  0.8629342 , ..., -1.1875995 ,\n",
       "        -1.102472  , -1.0981644 ],\n",
       "       ...,\n",
       "       [ 0.28267604,  1.2635132 ,  1.6339209 , ..., -1.0381713 ,\n",
       "        -1.0361733 , -1.030344  ],\n",
       "       [-1.0513521 , -1.1900522 , -1.4500262 , ...,  1.1392133 ,\n",
       "         0.22349997, -0.5103869 ],\n",
       "       [ 0.2596755 ,  0.19007836, -0.01513426, ...,  1.651539  ,\n",
       "         1.7041686 ,  0.8912365 ]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardizing Training Data\n",
    "scaler1 = skp.StandardScaler(copy=False).fit(X_train)\n",
    "scaler1.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.1705022 ,  1.096167  ,  0.7910445 , ..., -1.2601049 ,\n",
       "        -1.2511982 , -1.2851127 ],\n",
       "       [ 1.5094512 ,  1.2260702 ,  0.99877656, ..., -0.44840354,\n",
       "        -0.40823504, -0.49670002],\n",
       "       [-0.88578564, -0.39772037,  0.04320839, ..., -0.6697766 ,\n",
       "        -0.60091233, -0.6400478 ],\n",
       "       ...,\n",
       "       [-0.6372234 , -1.0905375 , -1.2447313 , ..., -1.0879259 ,\n",
       "        -0.8899283 , -0.37724352],\n",
       "       [ 0.2892361 , -0.07296232,  0.27171376, ...,  0.58467096,\n",
       "         0.4828974 ,  0.45895174],\n",
       "       [ 0.78636074,  0.59820455,  0.5417655 , ...,  0.11732771,\n",
       "        -0.986267  , -0.71172166]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardizing Test Data\n",
    "scaler2 = skp.StandardScaler(copy=False).fit(X_test)\n",
    "scaler2.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Covariance of Train Data\n",
    "# Take Transpose of Training Data before calculating covariance\n",
    "# Bcoz we want variability between features i.e. between columns not between rows\n",
    "X_train_cov = np.cov(X_train.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4096, 4096)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cov.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.0033444 ,  0.94744189,  0.8164271 , ..., -0.16278   ,\n",
       "        -0.18061455, -0.14454212],\n",
       "       [ 0.94744189,  1.00334382,  0.92200701, ..., -0.24200364,\n",
       "        -0.25745395, -0.21128855],\n",
       "       [ 0.8164271 ,  0.92200701,  1.00334427, ..., -0.34298165,\n",
       "        -0.34057894, -0.2907474 ],\n",
       "       ...,\n",
       "       [-0.16278   , -0.24200364, -0.34298165, ...,  1.00334438,\n",
       "         0.92101021,  0.83811675],\n",
       "       [-0.18061455, -0.25745395, -0.34057894, ...,  0.92101021,\n",
       "         1.00334467,  0.9471965 ],\n",
       "       [-0.14454212, -0.21128855, -0.2907474 , ...,  0.83811675,\n",
       "         0.9471965 ,  1.00334455]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Eigen Values & it's corresponding Eigen Vectors\n",
    "eig_values, eig_vectors = LA.eig(X_train_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1122.8981483742148+0j)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eig_values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00408696+0.j, -0.0054459 +0.j, -0.00697279+0.j, ...,\n",
       "        0.00152933+0.j,  0.00385955+0.j,  0.00323643+0.j])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eig_vectors[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4096,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eig_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4096, 4096)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eig_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a Tuple as (Eigen Value, Eigen Vector) & Sort it\n",
    "eig_pairs = [(np.abs(eig_values[i]), eig_vectors[:,i]) for i in range(len(eig_values))]\n",
    "eig_pairs.sort(key=lambda x: x[0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1122.8981483742148"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eig_pairs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Threshold Upon Which We Can Decide Whether To Keep Or Discard Which Feature Vector.\n",
    "exp_var_percentage = 90.0\n",
    "\n",
    "tot = sum(eig_values)\n",
    "var_exp = [(i / tot)*100 for i in sorted(eig_values, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "\n",
    "num_vec_to_keep = 0\n",
    "\n",
    "# Simply Count The Number Of Feature Vectors Which We Would Like To Keep Based On A Selected Threshold of 90%.\n",
    "for index, percentage in enumerate(cum_var_exp):\n",
    "    if percentage > exp_var_percentage:\n",
    "        num_vec_to_keep = index + 1\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_vec_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Our Data Onto The Vectors We Decided To Keep. We Do This By Building A Projection Matrix.\n",
    "num_features = X_train.shape[1]\n",
    "proj_mat = eig_pairs[0][1].reshape(num_features,1)\n",
    "for eig_vec_idx in range(1, num_vec_to_keep):\n",
    "    proj_mat = np.hstack((proj_mat, eig_pairs[eig_vec_idx][1].reshape(num_features,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4096, 61)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recast The Data Along The Principal Components Axes\n",
    "X_train_PCA = X_train @ (proj_mat)\n",
    "X_test_PCA = X_test @ (proj_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 61)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_PCA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting Only Real Part of Eigen Vectors\n",
    "X_train_PCA = np.real(X_train_PCA)\n",
    "X_test_PCA = np.real(X_test_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally Applying Any Of The Classification Methods For Classifying The Data\n",
    "classifier = LogisticRegression(solver = 'liblinear', multi_class = 'ovr')\n",
    "classifier.fit(X_train_PCA, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Predictions\n",
    "Y_pred = classifier.predict(X_test_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         4\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       1.00      1.00      1.00         4\n",
      "           3       0.67      1.00      0.80         2\n",
      "           4       1.00      1.00      1.00         3\n",
      "           5       1.00      1.00      1.00         2\n",
      "           7       1.00      1.00      1.00         2\n",
      "           8       1.00      1.00      1.00         2\n",
      "           9       1.00      1.00      1.00         2\n",
      "          10       1.00      1.00      1.00         1\n",
      "          11       1.00      1.00      1.00         4\n",
      "          12       1.00      0.80      0.89         5\n",
      "          13       1.00      1.00      1.00         1\n",
      "          14       1.00      1.00      1.00         1\n",
      "          15       1.00      1.00      1.00         4\n",
      "          17       1.00      1.00      1.00         1\n",
      "          18       1.00      1.00      1.00         2\n",
      "          19       1.00      1.00      1.00         5\n",
      "          20       1.00      1.00      1.00         1\n",
      "          21       1.00      1.00      1.00         2\n",
      "          22       1.00      1.00      1.00         1\n",
      "          23       1.00      0.80      0.89         5\n",
      "          24       0.50      1.00      0.67         1\n",
      "          25       1.00      1.00      1.00         4\n",
      "          26       1.00      1.00      1.00         3\n",
      "          27       1.00      1.00      1.00         3\n",
      "          28       1.00      1.00      1.00         4\n",
      "          29       1.00      1.00      1.00         4\n",
      "          30       1.00      1.00      1.00         4\n",
      "          31       1.00      1.00      1.00         2\n",
      "          32       1.00      1.00      1.00         2\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       1.00      1.00      1.00         2\n",
      "          35       1.00      1.00      1.00         3\n",
      "          36       1.00      1.00      1.00         4\n",
      "          37       1.00      1.00      1.00         3\n",
      "          38       1.00      1.00      1.00         1\n",
      "          39       1.00      1.00      1.00         2\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       100\n",
      "   macro avg       0.98      0.99      0.98       100\n",
      "weighted avg       0.99      0.98      0.98       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing Classification Report\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 0 0 ... 0 0 0]\n",
      " [0 2 0 ... 0 0 0]\n",
      " [0 0 4 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 3 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 2]]\n"
     ]
    }
   ],
   "source": [
    "# Printing Confusion Matrix\n",
    "print(confusion_matrix(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98\n"
     ]
    }
   ],
   "source": [
    "# Printing Accuracy\n",
    "print(accuracy_score(Y_test, Y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
